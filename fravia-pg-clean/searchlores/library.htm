<html>
<head>
<!-- web searching lore: pagename begin -->
<title>library.htm: the searchers' Library, which will fill the reading wants of any keen seeking-minded searcher</title>
<LINK REL="SHORTCUT ICON" HREF="images/favicon.ico">
<!-- web searching lore: pagename end -->
<meta http-equiv="Content-Type"		content="text/html; charset=iso-8859-1">
<meta http-equiv="Content-Style-Type"	content="text/css">
<meta http-equiv="Content-Script-Type"	content="text/javascript">
<meta name="description"	content="This site teaches basic and advanced search techniques for people interested in finding any kind of information on the web. Here are informations, documents, links, etc. related to web-searching">
<meta name="keywords"		content="searching, combing, klebing, how to search, search techniques, hints and tips for searching the web, How do I learn to search?, Advanced Internet searching, How do I search the web?, fravia+">
<meta name="author" content="fravia+">
<meta name="copyright"		content="Copyright(c) 1952-2032 fravia+">

<STYLE type="text/css">
A:link { color: #02F }
A: visited  { color: #808 }
A:hover { color: purple; background: #AFB }
</STYLE>
</head>
<BODY bgcolor="#CCCCCC" TEXT=#001010 LINK="#0000FF" ALINK="#00FF00" VLINK="#3366CC">
<center>
<table border="1" width=88%><TR><TD width="37%"><FONT size=-1 align="left">&nbsp;  <A target="_blank"  
      href="index.html">portal</A> &#8594; <a target="_blank" href="classroo.htm">searching classrooms</a>  &#8594;  
      library.htm</FONT>
<TD><div align="center">&nbsp;You'r deep inside <a href="http://www.searchlores.org">searchlores</a>&nbsp;</div></TD>
<TD width="37%" bgcolor="#cccccc" align=right><a href="rose.htm"><img src="images/windrose.png" alt="This is a windrose" align="middle" border="0" height="48" hspace="0" vspace="0" width="48"></a>  
</TD></TR></table>
<br></center>

<!-- library, begin --> 
<table border="0"><tr><td rowspan=2>

<a href="classroo.htm"><IMG SRC="images/spitzweg2.jpg" ALT="Buecherwurm" ALIGN=BOTTOM 
WIDTH="247" HEIGHT="467" BORDER=0 VSPACE=0 HSPACE=0></a>

</td><td>&nbsp;&nbsp;&nbsp;</td><td><center><font size=+3>The searchers' library</font><hr width=33%>
<font size=+1>
<i>The searchers' Library will fill the reading wants 
of any keen, seeking-minded searcher</i><hr width=88%></center></font></td></tr><tr><td>&nbsp;
&nbsp;&nbsp;&nbsp;</td><td><font size=+1><center>You would be well advised to learn some <a href="evaluate.htm">evaluation lore</a>: 
the fact that an essay is in pdf format and full of references to other "university" 
essays, the fact that it's language appears more formal, more "professional" 
does not mean NOTHING. As you will realize on yourself, many of
  <a href="essays.htm">our own essays</a> beat, 
for importance, depth of reach and development 
potential these "pdf-library" essays hands down. Yet some "established" papers  
are indeed useful and instructive.<br>As always, 
<i>caveat emptor</i> (and <i>gaudet fur</i>&nbsp;:-) Judge by yourself.
<br>
      Pdf 
files may be cracked through some of <a href="pdffing.htm">these tricks</a>, 
our Library is in progress, your suggestions for inclusion are welcome.
<br></center><hr width=78%>       

<table border="0"><tr><td><font size=+2>The library:</font></td><td colspan=2><font size=+2><center>    
<IMG SRC="images/bulletr.gif" ALT="red" align="bottom" valign="bottom" WIDTH="13" 
HEIGHT="13">&nbsp;<a href="library.htm#libra">"Pdf" texts</a>&nbsp;<IMG SRC="images/bulletr.gif" ALT="red" align="bottom" valign="bottom" WIDTH="13" 
HEIGHT="13">&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
</td><td colspan=2><font size=+2><center><IMG SRC="images/bulletr.gif" ALT="red" align="bottom" valign="bottom" WIDTH="13" 
HEIGHT="13">&nbsp;<a href="library.htm#htmlfi">Html texts</a>&nbsp;<IMG SRC="images/bulletr.gif" ALT="red" align="bottom" valign="bottom" WIDTH="13" 
HEIGHT="13">&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</td></tr><tr><td>
          <font size=+1>Our essays:</font></td><td><center>  
<a href="essays.htm">searching</a>
</td><td><center>
 <a href="pro_essa.htm">proxing</a> 
</td><td><center>
 <a href="malware.htm#essays">malwares</a>   
</td><td><center>
&nbsp; 
</td></tr><tr><td>
 
          <font size=+1>Our classes and labs:&nbsp;&nbsp;&nbsp;&nbsp;</font></td><td><center>                                  
   <a href="c_intro.htm">classrooms</a> 
</td><td><center> 
 <a href="lab1.htm">Lab 1</a>
</td><td><center> 
 <a href="lab2.htm">Lab 2</a>
</td><td><center> 
 <a href="lab3.htm">Lab 3</a>    
<br>
</td></tr>
<tr><td colspan=5><hr width=78%>
           <font size=+1>
<IMG SRC="images/bulletr.gif" ALT="red" align="bottom" valign="bottom" WIDTH="13" 
HEIGHT="13">&nbsp;<a href="discudis.htm">A discussion about the utility of this library</a></font>: "The major problem I see with these papers is that they 
are made to sound pompous in order to impress the other empty 'academic' heads. For a 
person that just wants to make the damn thing work they are tedious to read, hard to quickly 
evaluate and can probably be replaced with an hour or so serious thinking on the problem. Now, 
with the more avant-garde problems they may be the only source of reliable knowledge, which sadly 
means that one has to swallow the tons of crap to find the important bits in there, but hey - nobody 
said it should be easy :)"</td></tr></table>
             </font>

</table>
<br><br>
<a name="libra"></a> 
<center>
<table border=1 width=88%><tr><td bgcolor="#C6E7C6"><center><font
size=+3>The Library (pdf)</font></center></td></tr></table>
</center>
<br><br>    

                                 
Olivier Chapelle: <a href="library/Large&#32;margin&#32;optimization&#32;of&#32;ranking&#32;measures.pdf">
Large margin optimization of ranking measures</a> (2007)<br>
"WMost ranking algorithms, such as pairwise ranking, are based on the op-
timization of standard loss functions, but the quality measure to test web
page rankers is often different. We present an algorithm which aims at op-
timizing directly one of the popular measures, the Normalized Discounted
Cumulative Gain. It is based on the framework of structured output learn-
ing, where in our case the input corresponds to a set of documents and the
output is a ranking. The algorithm yields improved accuracies on several
public and commercial ranking datasets.
"  

<hr width=44% align=left> 

   
Jon M. Kleinberg: <a href="library/authoratitativesources.pdf">
Authoritative Sources in a Hyperlinked Environment</a> (1997-1998)<br>
"We develop a set of algorithmic tools for extracting information from
the link structures of such environments, and report on experiments that demonstrate
their e.ectiveness in a variety of contexts on the World Wide Web. The central issue
we address within our framework is the distillation of broad search topics, through
the discovery of authoritative information sources on such topics."  

<hr width=44% align=left> 
L. Page; S. Brin; R. Motwani; T. Winograd: <a href="library/PageRankCitation.pdf">
The PageRank Citation Ranking: Bringing Order to the Web</a> (January 1998) <br>
"The importance of a Web page is an inherently subjective matter, which depends on the
readers interests, knowledge and attitudes. But there is still much that can be said objectively
about the relative importance of Web pages. This paper describes PageRank, a method for
rating Web pages objectively and mechanically, effectively measuring the human interest and
attention devoted to them." 

<hr width=44% align=left>   
Steve Lawrence, C. Lee Giles: <a href="library/search-science98.pdf">
Searching the World Wide Web</a> (1998)  <br>
"The coverage of any one engine is significantly limited:
No single engine indexes more than about one-third of the “indexable Web,” the coverage
of the six engines investigated varies by an order of magnitude, and combining
the results of the six engines yields about 3.5 times as many documents on average as
compared with the results from only one engine."     

<hr width=44% align=left>   
Min Fang, Narayanan Shivakumar, Hector Garcia-Molina, & VV.AA:

<a href="library/iceberg.pdf">Computing Iceberg Queries Efficiently</a>  (Aug 1998)
<br>
"In this paper we develop efficient execution strategies for an important class of queries 
that we
call iceberg queries. An iceberg query performs an aggregate function 
over an attribute (or set
of attributes) and then eliminates aggregate values that are below some 
specified threshold." 

<hr width=44% align=left> 
Brian D. Davison et alia: <a href="library/discoweb_teoma.pdf">
DiscoWeb: Applying Link Analysis to Web Search</a> (1998)<br>
"How often does the search engine of your choice produce results that are less than satisfying, generating endless
links to irrelevant pages even though those pages may contain the query keywords? How often are you given pages
that tell you things you already know?"

<hr width=44% align=left> 
VV.AA: <a href="library/sdk_v26.pdf">
AltaVista Search Intranet Developer's Kit</a> (Reference manual, Ver. 2.6, April 1999)<br>
"The AltaVista Search Developer's Kit Version lets you build your own search and retrieval
application or add AltaVista Search-powered search capabilities to database applications and
file repositories. Users can find what they need quickly and easily without special database
training."
             

                      

<hr width=44% align=left> 
Steinowitz, <i>Hidden Internet connections</i>:<br>Part 1: <a href="library/sw_hic1.pdf">Windows and Internet connections</a>, 
&nbsp;&nbsp;Part 2: <a href="library/sw_hic2.pdf">Intermezzo - The Internet</a> (November 1999)
<br>
"It's often said that Microsoft writes certain hidden features in its software which
automatically connects to Microsoft servers to transmit data about the user. All
this incoming information is stored in huge databases which Microsoft subsequently
uses for its merciless tactics in maintaining its world-dominating position.
<br>How can we prove that Microsoft is doing this?"
       
<hr width=44% align=left>   
G. W. Flake, S. Lawrence, C. Lee Giles, F. M. Coetzee:  <a href="library/flake-ieee2002.pdf">
Self-Organization of the Web and Identification of Communities</a> (1999) <br>
"Despite the decentralized and unorganized nature of the web, we show that the web self-organizes
such that communities of highly related pages can be efficiently identified based purely on connectivity.
This discovery allows the identification of communities independent of, and unbiased by, the specific
words used by authors."
  
<hr width=44% align=left>   
B.J. Jansen, A. Spink, T. Saracevic,:  <a href="library/ipm98.pdf">
Real Life, Real Users, and Real Needs: A Study and Analysis of User 
Queries on the Web</a> (2000) <br>
"We analyzed transaction logs containing 51,473 queries posed by 18,113 users of 
Excite, a major
Internet search service. We provide data on: (i) sessions - changes in queries 
during a session, 
number of pages
viewed, and use of relevance feedback, (ii) queries - the number of search terms, 
and the use of logic and
modifiers, and (iii) terms - their rank/frequency distribution and 
the most highly used search terms."

<hr width=44% align=left>               
Gary William Flake, Steve Lawrence, C. Lee Giles:  <a href="library/web-kdd00.pdf">
Efficient Identification of Web Communities</a> (2000)<br>
"We define a community on the web as a set of sites that have more links 
(in either direction) to members of the community than to non-members. 
Members of such a community can be efficiently identified in a maximum flow / 
minimum cut framework, where the source is composed of 
known members, and the sink consists of well-known non-members. "
  
<hr width=44% align=left>   
Jason Zien et alia: <a href="library/1077.pdf">
Web Query Characteristics and their Implications on Search Engines</a> (2000)

""   

    
  <hr width=44% align=left>   
Stephen Dill et alia: <a href="https://searchlores.nickifaulk.com/library/P069.pdf">
Self-similarity in the Web</a> (2001)

"Algorithmic tools for searching and mining the web are becoming increasingly 
sophisticated and vital. In this context, algorithms which use and exploit 
structural information about the web perform better than generic methods 
in both efficiency and reliability. We present an extensive characterization 
of the graph structure of the web, with a view to enabling 
high-performance applications that make use of this structure"   
            

<hr width=44% align=left> 
 VV.AA: <a href="library/ISS204.pdf">
AltaVista Internet Search Services</a> (Search Engine Interface Description, Ver 2.04, July 2000)<br>
"AltaVista’s Internet Search Services (ISS) program is designed to provide global
 search capabilities for
Internet portals. ISS allows portal operators to provide their 
end users with access to AltaVista’s
collection of images, audio clips, video clips, and general text search results."

 <hr width=44% align=left>     Juan M. Madrid, Susan Gauch:
<a href="library/incorporating.pdf">Incorporating Conceptual Matching in Search</a><br>
"As the number of available Web pages grows, users experience increasing 
difficulty finding documents relevant to their interests. One of the 
underlying reasons for this is that most search engines find matches 
based on keywords, regardless of their meanings. To provide the user 
with more useful information, we need a system that includes information 
about the conceptual frame of the queries as well as its keywords."                                  

<hr width=44% align=left>   
A. Arasu, J. Cho, H. Garcia-Molina, A. Paepcke, S.Raghavan: <a href="library/searchingtheweb.pdf">
Searching the Web</a> (2001)       <br>
"We offer an overview of current Web search engine design. After introducing a generic
search engine
architecture, we examine each engine component in turn. We cover crawling, local 
Web page storage,
indexing, and the use of link analysis for boosting search performance."                 


<hr width=44% align=left>   
Steve Lawrence et alia:  <a href="library/persistence-computer01.pdf">
Persistence of Web References in Scientific Research</a> (February 2001) <br>
"Some argue that the lack of persistence of web resources
means that they should not be cited in scientific research. We analyze references to web resources in
computer science publications, finding that the number of web references has increased dramatically in
the last few years, and that many of these references are now invalid."

<hr width=44% align=left>   
Jon Kleinberg & Steve Lawrence:  <a href="library/structureoftheweb.pdf">
The Structure of the Web</a> (November 2001)<br>
"Because of the decentralized nature of
its growth, the Web has been widely be-
lieved to lack structure and organization as
a whole. Recent research, however, shows
a great deal of self-organization."

<hr width=44% align=left>   
Anukool Lakhina et alia:  <a href="library/2002-015-internet-geography.pdf">
On the Geographic Location of Internet Resources</a> (2001)<br>
"One relatively unexplored question about the
Internet’s physical structure concerns the geographical location
of its components: routers, links and autonomous
systems (ASes)."

<hr width=44% align=left>   
Robert Steele:  <a href="library/techniques-for-specialized-search.pdf">
Techniques for Specialized Search Engines</a> (2001) <br>
"It is emerging that it is very difficult for the
major search engines to provide a comprehensive
and up-to-date search service of the Web. Even the
largest search engines index only a small
proportion of static Web pages and do not search
the Web’s backend databases that are estimated to
be 500 times larger than the static Web."
 
<hr width=44% align=left>   
Steve Lawrence:  <a href="library/context-deb00.pdf">
Context in Web Search</a> (2001)   <br>
"Web search engines generally treat search requests
in isolation. The results for a given query
are identical, independent of the user, or the context
in which the user made the request. Next generation
search engines will make increasing
use of context information, either by using explicit
or implicit context information from users, or by
implementing additional functionality within restricted
contexts. "

<hr width=44% align=left>   
Jean-Pierre Eckmann, Elisha Mosesy: <a href="library/0110338.pdf">
Curvature of Co-Links Uncovers Hidden Thematic Layers in the WorldWideWeb</a> (30 November 2001)
<br>
"I read some papers about the subject and I think this one really captures the essence of web community 
and IMO gives an efficient (focused) way of spidering them", (<font color=blue>Nemo</font>)

<hr width=44% align=left>   
Ding Choon Hoong, Rajkumar Buyya:
 <a href="library/guidedgoogle.pdf">Guided Google: A Meta Search Engine and its Implementation using 
the Google Distributed Web Services</a>  
<br>
"This paper proposes a guided meta-search engine, called “Guided Google”, which provides meta-search
capability developed using the Google Web Services. It guides and allows the user to view the search
results with different perspectives. This is achieved through simple manipulation and automation of the
existing Google functions. Our meta-search engine supports search based on “combinatorial keywords” and
“search by hosts"

<hr width=44% align=left> 
Wolfgang Barthel, Alexander K. Hartmann, Martin Weigt:
 <a href="library/localsearch.pdf">Solving satisfiability problems by fluctuations: 
The dynamics of stochastic local search algorithms</a> (2002)  
<br>
"In order to find the exact shortest path, a broadcast method equivalent to a breadth 
first search (BFS) must be used.
As mentioned in our discussion of Gnutella, broadcasting can overwhelm the 
bandwidth resources of the network."

<hr width=44% align=left> 
        David Wolpert, Kagan Tumer, Esfandiar Bandari:        
 <a href="library/intelligentcoordinates.pdf">Improving Search Algorithms by Using Intelligent 
Coordinates</a>     (23 Jan 2003)
<br>
"We consider the problem of designing a set of computational agents so that as they all pursue
their self-interests a global function G of the collective system is optimized. Three factors govern the
quality of such design. The first relates to conventional exploration-exploitation search algorithms
for finding the maxima of such a global function, e.g., simulated annealing (SA)." 
 <hr width=44% align=left> 

 Vladimir Pestov, Aleksandar Stojmirovic
 <a href="library/indexingschemes.pdf">Indexing schemes for similarity search: 
an illustrated paradigm</a>  (14 Nov 2002)
<br>
"What is needed, is a fully developed mathematical
paradigm of indexability for similarity search that
would incorporate the existing structures of database
theory and possess a predictive power."

   <hr width=44% align=left> 
Lada A. Adamic, Rajan M. Lukose, Bernardo A. Huberman:
<a href="library/localsearch.pdf">Local Search in Unstructured Networks</a>  (4 Jun 2002)
<br>
"It has become clear that the simplest classical model of random networks, the
Erdos-Renyi model, is inadequate for describing the topology of many naturally 
occurring networks. These diverse
networks are more accurately described by power-law or scale-free link 
distributions. In these highly skewed distributions,
the probability that a node has k links is approximately proportional to 1=kˆT"

     
 <hr width=44% align=left> 
Andrea Montanari, Riccardo Zecchina: 

<a href="library/rareevents.pdf">Boosting search by rare events</a>     (19 Dec 2001)
<br>
"Randomized search algorithms for hard combinatorial problems exhibit a 
large variability of performances.
We study the different types of rare events which occur in such out-of-equilibrium
stochastic processes and we show how they cooperate in determining the 
final distribution of running
times. As a byproduct of our analysis we show how search algorithms are optimized by random
restarts."

  
<hr width=44% align=left>            
Tair-Rong Sheu, Kathleen Carley:
<a href="library/monopolypower.pdf">Monopoly Power on the Web - A Preliminary 
Investigation of Search Engines</a>           (27 Oct 2001)<br>
"We focus on major search engines that provide general search services. We assume
that the top 19 search engines in the June 2000 rating from Nielsen/NetRatings account
for 100 % market share. We collected data on the hyperlinks connecting these search
engine web sites over a five-month period from August 12th 2000 to Dec. 12th 2000. Each
month’s network was stored as a binary matrix."     

<hr width=44% align=left> 
Hung-Yu Kao, Shian-Hua Lin, Jan-Ming Ho, Ming-Syan Chen:
<a href="library/distillingWebPages4Clarity.pdf">Mining Web Informative Structures and Contents 
Based on Entropy Analysis</a>        (2001)
<br>
"In this paper, we study the problem of mining the informative structure of a news Web site that consists of
thousands of hyperlinked documents. We define the informative structure of a news Web site as a set of index
pages (or referred to as TOC, i.e., table of contents, pages) and a set of article pages linked by these TOC pages.
Based on the Hyperlink Induced Topics Search (HITS) algorithm, we propose an entropy-based analysis (LAMIS)
mechanism for analyzing the entropy of anchor texts and links to eliminate the redundancy of the hyperlinked
structure so that the complex structure of a Web site can be distilled."

<hr width=44% align=left>  
Brian Amento, Loren Terveen, and Will Hill:               
<a href="library/doesauthority.pdf">Does "Authority" Mean Quality? Predicting Expert Quality Ratings
of Web Documents</a>   (2000)<br>
"Search engines like Google or AltaVista return tens of
thousands of items, and even human-maintained
directories like Yahoo or UltimateTV contain dozens
to hundreds of items.
However, these items vary widely in quality, ranging
from large, well-maintained sites to smaller sites that
contain specialized content to nearly content-free,
completely worthless sites. No one has the time to
wade through more than a handful of items."

                  
 <hr width=44% align=left>    Weiyi Meng and alia: 
<a href="library/p310-meng.pdf">A Highly Scalable and Effective Method
for Metasearch</a> (2001)<br>
"A metasearch engine is a system that supports unified access to multiple local search engines.
Database selection is one of the main challenges in building a large-scale metasearch engine. The
problem is to efficiently and accurately determine a small number of potentially useful local search
engines to invoke for each user query. In order to enable accurate selection, metadata that reflect
the contents of each search engine need to be collected and used. This article proposes a highly
scalable and accurate database selection method."

                               
 <hr width=44% align=left>    Jonathan D. Herbach: 
<a href="library/p310-meng.pdf">Improving Authoritative Sources in a
Hyperlinked Environment via
Similarity Weighting
Or, "How to get better search results on the Web"</a> (May 2001)<br>
"Recent literature demonstrates that the network structure of a hyperlink environment can
serve as an e.ective source for inferring the importance of content in documents. One
such connectivity-analysis algorithm, HITS, determines document importance based upon
the hyperlink structure of the Web. In order to compensate for the problems of a pure
connectivity-analysis algorithm, we develop and test an algorithm based upon HITS that
also considers document content."




                               
   <hr width=44% align=left>    Taher H. Haveliwala: 
<a href="library/topicsen.pdf">TopicSensitive PageRank</a> (2002)<br>
"In the original PageRank algorithm for improving the ranking of 
search-query results, a single PageRank vector is computed, using the link 
structure of the Web, to capture the
relative \ importance" ofWeb pages, independent of any particular search query. 
To yield more accurate search results,
we propose computing a set of PageRank vectors, biased using a set 
of representative topics, to capture more accurately
the notion of importance with respect to a particular topic."
                      
   <hr width=44% align=left>    Amr Z. Kronfol: 
<a href="library/kronfol_final_thesis.pdf">FASD: A Fault-tolerant, Adaptive, Scalable,
Distributed Search Engine</a> (Mai 2002)<br>
"Search and retrieval under the Napster model; 
Search and retrieval under the Gnutella model; Retrieval under the Freenet model.

This paper introduces FASD, a fault-tolerant, adaptive, scalable, and distributed
search layer designed to augment existing peer-to-peer applications.
 Although completely decentralized, FASD’s approach
is able to efficiently match the recall and precision of a centralized search engine."

    <hr width=44% align=left> Sepandar D. Kamvar, Taher H. Haveliwala, 
Christopher D. Manning, Gene H. Golub: 

<a href="library/pagera.pdf">Exploiting the Block Structure of theWeb for Computing
PageRank</a> (2003)<br>  
"The web link graph has a nested block structure: the vast majority of 
hyperlinks link pages on a host to other pages on the same host, 
and many of those that do not link pages within the same domain. 
We show how to exploit this structure to speed up the computation 
of PageRank by a 3-stage algorithm whereby (<font color=blue>1</font>)~the local PageRanks 
of pages for each host are computed independently using the link 
structure of that host, (<font color=blue>2</font>)~these local PageRanks are then weighted 
by the "importance" of the corresponding host, and 
(<font color=blue>3</font>)~the standard PageRank algorithm is then run using 
as its starting vector the weighted aggregate of the 
local PageRanks. Empirically, this algorithm speeds 
up the computation of PageRank by a factor of 2 in 
realistic scenarios. Further, we develop a variant 
of this algorithm that efficiently computes many different 
"personalized" PageRanks, and a variant that efficiently 
recomputes PageRank after node updates."
      <hr width=44% align=left>Tara Calishain and Rael Dornfest:  
 
OReilly_-_Google_Hacks  
(2003)<br><br><font color=blue>Google Hacks: 100 Industrial-Strength 
Tips and Tools</font>, is a book of tips about Google,
 the currently foremost Internet search engine,
 by Tara Calishain and Rael Dornfest (ISBN 0-596-00447-8). 
The book was published by O'Reilly & Associates on February 2003. Very useful, do find it on the web, or buy it.<br>
<br>Table of Contents<br>
Credits
Foreword
Preface
Chapter 1. Searching Google
1. Setting Preferences
2. Language Tools
3. Anatomy of a Search Result
4. Specialized Vocabularies: Slang and Terminology
5. Getting Around the 10 Word Limit
6. Word Order Matters
7. Repetition Matters
8. Mixing Syntaxes
9. Hacking Google URLs
10. Hacking Google Search Forms
11. Date-Range Searching
12. Understanding and Using Julian Dates
13. Using Full-Word Wildcards
14. inurl: Versus site:
15. Checking Spelling
16. Consulting the Dictionary
17. Consulting the Phonebook
18. Tracking Stocks
19. Google Interface for Translators
20. Searching Article Archives
21. Finding Directories of Information
22. Finding Technical Definitions
23. Finding Weblog Commentary
24. The Google Toolbar
25. The Mozilla Google Toolbar
26. The Quick Search Toolbar
27. GAPIS
28. Googling with Bookmarklets
Chapter 2. Google Special Services and Collections
29. Google Directory
30. Google Groups
31. Google Images
32. Google News
33. Google Catalogs
34. Froogle
35. Google Labs
Chapter 3. Third-Party Google Services
36. XooMLe: The Google API in Plain Old XML
37. Google by Email
38. Simplifying Google Groups URLs
39. What Does Google Think Of...
40. GooglePeople
Chapter 4. Non-API Google Applications
41. Don't Try This at Home
42. Building a Custom Date-Range Search Form
43. Building Google Directory URLs
44. Scraping Google Results
45. Scraping Google AdWords
46. Scraping Google Groups
47. Scraping Google News
48. Scraping Google Catalogs
49. Scraping the Google Phonebook
Chapter 5. Introducing the Google Web API
50. Programming the Google Web API with Perl
51. Looping Around the 10-Result Limit
52. The SOAP::Lite Perl Module
53. Plain Old XML, a SOAP::Lite Alternative
54. NoXML, Another SOAP::Lite Alternative
55. Programming the Google Web API with PHP
56. Programming the Google Web API with Java
57. Programming the Google Web API with Python
58. Programming the Google Web API with C# and .NET
59. Programming the Google Web API with VB.NET
Chapter 6. Google Web API Applications
60. Date-Range Searching with a Client-Side Application
61. Adding a Little Google to Your Word
62. Permuting a Query
63. Tracking Result Counts over Time
64. Visualizing Google Results
65. Meandering Your Google Neighborhood
66. Running a Google Popularity Contest
67. Building a Google Box
68. Capturing a Moment in Time
69. Feeling Really Lucky
70. Gleaning Phonebook Stats
71. Performing Proximity Searches
72. Blending the Google and Amazon Web Services
73. Getting Random Results (On Purpose)
74. Restricting Searches to Top-Level Results
75. Searching for Special Characters
76. Digging Deeper into Sites
77. Summarizing Results by Domain
78. Scraping Yahoo! Buzz for a Google Search
79. Measuring Google Mindshare
80. Comparing Google Results with Those of Other Search Engines
81. SafeSearch Certifying URLs
82. Syndicating Google Search Results
83. Searching Google Topics
84. Finding the Largest Page
85. Instant Messaging Google
Chapter 7. Google Pranks and Games
86. The No-Result Search (Prank)
87. Google Whacking
88. GooPoetry
89. Creating Google Art
90. Google Bounce
91. Google Mirror
92. Finding Recipes
Chapter 8. The Webmaster Side of Google
93. A Webmaster's Introduction to Google
94. Generating Google AdWords
95. Inside the PageRank Algorithm
96. 26 Steps to 15K a Day
97. Being a Good Search Engine Citizen
98. Cleaning Up for a Google Visit
99. Getting the Most out of AdWords
100. Removing Your Materials from Google
Index
        
        
        <hr width=44% align=left>
Thomas Demuth:
<a href="https://searchlores.nickifaulk.com/library/pet02.pdf">
A Passive Attack on the Privacy of Web Users 
Using Standard Log Information</a><br> 
(it's always kinda suspect when they do not put any date. Around 2002)<br>                             
                              
Abstract: Several active attacks on user privacy in the World Wide
Web using cookies or active elements (Java, Javascript, ActiveX) are
known. One goal is to identify a user in consecutive Internet session to
track and to profile him (such a profile can be extended by personal
information if available).
In this paper, a passive attack is presented that uses information of a
dierent network layer in the first place. It is exposed how expressive the
data of the HyperText Transfer Protocol (HTTP) can be with respect to
identify computers (and therefore their users). An algorithm to reidentify
computers using dynamically assigned IP addresses with a certain
degree of assurance is introduced. Thereafter simple countermeasures are
demonstrated.
The motivation for this attack is to show the capability of passive privacy
attacks using Web server log files and to propagate the use of anonymising
techniques for Web users. <br>
Keywords: privacy, anonymity, user tracking and profiling, World Wide
Web, server logs
           <hr width=44% align=left>
     Caroline M. Eastman,
Bernard J. Jansen:    
    <a href="library/tois_2003.pdf">Coverage, Relevance, and Ranking:
The Impact of Query Operators on
Web Search Engine Results</a><br>
        (October 2003)
        <br>       
      Research has reported that about 10% of Web searchers utilize advanced query operators, with
the other 90% using extremely simple queries. It is often assumed that the use of query operators,
such as Boolean operators and phrase searching, improves the effectiveness of Web searching.
We test this assumption by examining the effects of query operators on the performance of three
major Web search engines. We selected one hundred queries from the transaction log of a Web
search service. Each of these original queries contained query operators such as AND, OR, MUST
APPEAR (C), or PHRASE (“ ”). We then removed the operators from these one hundred advanced
queries. We submitted both the original and modified queries to three major Web search engines;
a total of 600 queries were submitted and 5,748 documents evaluated. We compared the results
from the original queries with the operators to the results from the modified queries without
the operators. We examined the results for changes in coverage, relative precision, and ranking
of relevant documents. The use of most query operators had no significant effect on coverage,
relative precision, or ranking, although the effect varied depending on the search engine.We discuss
implications for the effectiveness of searching techniques as currently taught, for future information
retrieval system design, and for future research.     
           
             
             
              <hr width=44% align=left>
      Akamai Technologies (<I>The akamai clowns are sworn enemies of privacy: the original pdf file requires -theoretically-    
      a lengthy registration process and 
    is streamed using a very crude and old streaming technology, not exactly a propaganda tool for akamai dubious 
    feats, if you ask me</i>:-) Here a (bad) copy automatically downloaded using an image capture script plus automathical 
    scanner 
    (no human revision... you'll have to do it yourself).  
        <bR>
       <a href="library/akamai1.zip">Akamai Streaming - When Performance Matters</a> (January 2004)<br>
        (October 31, 2005,)
        <br> 
        
        This paper offers a quantitative study of streaming (delivering audio & video 
        over the Web) performance and demonstrates how Akamai ensures  streams 
        that "optimize return on streaming investments" (read: should avoid users copying the stream). 
        Akamai's streaming capabilities are 
        based on a unique combination of a global network of servers, innovative technology, 
        and a wealth of first-hand experience. In this paper, Akamai presents their streaming approach, 
        measurement methodology, and the data that demonstrates the "performance" of the Akamai Platform.
  <bR><br>
             
           <hr width=44% align=left> 
 Zoltàn Gyöngyi, Hector Garcia-Molina, Jan Pedersen:       
        
        <a href="library/2004-52.pdf">Combating Web Spam with TrustRank</a><br>
        (Proceedings of the 30th VLDB Conference, Toronto, Canada, 2004)
        <br>
        Web spam pages use various techniques to achieve
higher-than-deserved rankings in a search engine’s
results. While human experts can identify
spam, it is too expensive to manually evaluate a
large number of pages. Instead, we propose techniques
to semi-automatically separate reputable,
good pages from spam. We first select a small set
of seed pages to be evaluated by an expert. <br>Once
we manually identify the reputable seed pages, we
use the link structure of the web to discover other
pages that are likely to be good. In this paper
we discuss possible ways to implement the seed
selection and the discovery of good pages. We
present results of experiments run on the World
Wide Web indexed by AltaVista and evaluate the
performance of our techniques. Our results show
that we can effectively filter out spam from a significant
fraction of the web, based on a good seed
set of less than 200 sites.
      <hr width=44% align=left>
      Zoltan Gyongyi &nbsp; 
Hector Garcia-Molina: 
  
       <a href="https://searchlores.nickifaulk.com/library/Gyongyi.pdf">Web Spam Taxonomy</a><br>
        (SEARCH ENGINE SPAM WORKSHOP, May 09, 2005, Chiba, Japan)
        <br> 
        Web spamming refers to actions intended to mislead
search engines into ranking some pages higher than
they deserve. Recently, the amount of web spam has increased
dramatically, leading to a degradation of search
results. This paper presents a comprehensive taxonomy
of current spamming techniques, which we believe
can help in developing appropriate countermeasures.


<hr width=44% align=left>
     Albert Bifet, 

Carlos Castillo, 

Paul-Alexandru Chirita,
Ingmar Weber
  
       <a href="library/bifet.pdf">An Analysis of Factors Used in Search Engine Ranking</a><br>
        (SEARCH ENGINE SPAM WORKSHOP, May 09, 2005, Chiba, Japan)
        <br> 


This paper investigates the influence of different page
features on the ranking of search engine results. We use
Google (via its API) as our testbed and analyze the result
rankings for several queries of different categories
using statistical methods. We reformulate the problem
of learning the underlying, hidden scores as a binary
classification problem. To this problem we then apply
both linear and non-linear methods. In all cases, we
split the data into a training set and a test set to obtain
a meaningful, unbiased estimator for the quality of
our predictor. Although our results clearly show that
the scoring function cannot be approximated well using
only the observed features, we do obtain many interesting
insights along the way and discuss ways of obtaining
a better estimate and main limitations in trying to
do so.


<hr width=44% align=left>
     Panagiotis T. Metaxas, 
Joseph DeStefano
  
       <a href="library/metaxas.pdf">Web Spam, Propaganda and Trust</a><br>
        (SEARCH ENGINE SPAM WORKSHOP, May 09, 2005, Chiba, Japan)
        <br> 
In this paper, we first analyze the influence that web spam
has on the evolution of the search engines and we identify the
strong relationship of spamming methods to propagandistic
techniques in society. Our analysis provides a foundation to
understanding why spamming works and oers new insight
on how to address it. In particular, it suggest that one could
use anti-propagandistic techniques in the web to recognize
spam. The second part of the paper demonstrates such a
technique, called backwards propagation of distrust.

<hr width=44% align=left>
     Ricardo Baeza-Yates,
Carlos Castillo,
Vicente Lopez
  
       <a href="library/baeza-yates.pdf">Pagerank Increase under Different Collusion Topologies</a><br>
        (SEARCH ENGINE SPAM WORKSHOP, May 09, 2005, Chiba, Japan)
        <br> 

        We study the impact of collusion –nepotistic linking– in a
Web graph in terms of Pagerank. We prove a bound on the
Pagerank increase that depends both on the reset probability
of the random walk e and on the original Pagerank of
the colluding set. In particular, due to the power law distribution
of Pagerank, we show that highly-ranked Web
sites do not benefit that much from collusion.


<hr width=44% align=left>
     Baoning Wu, Brian D. Davison
  
       <a href="library/wu.pdf">Cloaking and Redirection: A Preliminary Studys</a><br>
        (SEARCH ENGINE SPAM WORKSHOP, May 09, 2005, Chiba, Japan)
        <br> 
        Cloaking and redirection are two possible search engine
spamming techniques. In order to understand
cloaking and redirection on the Web, we downloaded
two sets ofWeb pages while mimicking a popularWeb
crawler and as a common Web browser. We estimate
that 3% of the rst data set and 9% of the second
data set utilize cloaking of some kind. By checking
manually a sample of the cloaking pages from the second
data set, nearly one third of them appear to aim
to manipulate search engine ranking.<br>
We also examined redirection methods present in
the rst data set. We propose a method of detecting
cloaking pages by calculating the dierence of three
copies of the same page. We examine the dierent
types of cloaking that are found and the distribution
of different types of redirection.

<hr width=44% align=left>
     Sibel Adalý, Tina Liu, Malik Magdon-Ismail
  
       <a href="library/adali.pdf">Optimal Link Bombs are Uncoordinated</a><br>
        (SEARCH ENGINE SPAM WORKSHOP, May 09, 2005, Chiba, Japan)
        <br> 
        We analyze the recent phenomenon termed a Link
Bomb, and investigate the optimal attack pattern for
a group of web pages attempting to link bomb a specific
web page. The typical modus operandi of a link
bomb is to associate a particular page with a search
text and then boost that page’s pagerank. (The attacking
pages can only control their own content and
outgoing links.) Thus, when a search is initiated with
the text, a high prominence will be given to the attacked
page. We show that the best organization of
links among the attacking group to maximize the increase
in rank of the attacked node is the direct indi-
vidual attack, where every attacker points directly to
the victim and nowhere else. We also discuss optimal
attack patterns for a group that wants to hide itself
by not pointing directly to the victim. We quantify
our results with experiments on a variety of random
graph models.


<hr width=44% align=left>
     Gilad Mishne,
David Carmel, Ronny Lempel
  
       <a href="library/mishne.pdf">Blocking Blog Spam with Language Model Disagreement</a><br>
        (SEARCH ENGINE SPAM WORKSHOP, May 09, 2005, Chiba, Japan)
        <br> 
        We present an approach for detecting link spam common in
blog comments by comparing the language models used in
the blog post, the comment, and pages linked by the comments.
In contrast to other link spam filtering approaches,
our method requires no training, no hard-coded rule sets,
and no knowledge of complete-web connectivity. Preliminary
experiments with identification of typical blog spam
show promising results.


<hr width=44% align=left>
     András A. Benczúr, Károly Csalogány, Tamás Sarlós, Máté Uher
  
       <a href="library/benczur.pdf">SpamRank – Fully Automatic Link Spam Detection
Work in progress</a><br>
        (SEARCH ENGINE SPAM WORKSHOP, May 09, 2005, Chiba, Japan)
        <br> 
        
        Spammers intend to increase the PageRank of certain spam pages by creating a large number of links
pointing to them. We propose a novel method based on the concept of personalized PageRank that detects
pages with an undeserved high PageRank value without the need of any kind of white or blacklists or
other means of human intervention. We assume that spammed pages have a biased distribution of pages
that contribute to the undeserved high PageRank value. We define SpamRank by penalizing pages that
originate a suspicious PageRank share and personalizing PageRank on the penalties. Our method is
tested on a 31 M page crawl of the .de domain with a manually classified 1000-page stratified random
sample with bias towards large PageRank values.



<hr width=44% align=left>
      Zoltàn Gyöngyi, Pavel Berkhin, Hector Garcia-Molina, Jan Pedersen:  
  
       <a href="library/2005-33.pdf">Link Spam Detection Based on Mass
Estimation</a><br>
        (October 31, 2005,)
        <br> 
        
        Link spamming intends to mislead search engines and trigger an artificially high link-based
ranking of specific target web pages. This paper introduces the concept of spam mass, a
measure of the impact of link spamming on a page’s ranking. We discuss how to estimate
spam mass and how the estimates can help identifying pages that benefit significantly
from link spamming. In our experiments on the host-level Yahoo! web graph we use spam
mass estimates to successfully identify tens of thousands of instances of heavy-weight link
spamming.
  <bR><br><center>








<a href="https://searchlores.nickifaulk.com/speciallibrary.htm"><IMG SRC="images/library.jpg" ALT="Alma_Tadema" ALIGN=BOTTOM 
WIDTH="675" HEIGHT="484" BORDER=0 VSPACE=0 HSPACE=0></a>


  </center>

<!-- End Library  pdf -->
  

<br><br>
<a name="htmlfi"></a> 
<center>
<table border=1 width=88%><tr><td bgcolor="#C6E7C6"><center><font
size=+3>The Library (html)</font></center></td></tr></table>
</center>
<br><br> 

      
     <hr width=44% align=left> Jenny Edwards, Kevin McCurley, John Tomlin: 

<a href="library/An&#32;Adaptive&#32;Model&#32;for&#32;Optimizing&#32;Performance&#32;of&#32;an&#32;Incremental&#32;Web&#32;Crawler..htm">An Adaptive Model for Optimizing Performance 
of an Incremental Web Crawler</a> (2001)<br>   

"This paper outlines the design of a web crawler implemented for IBM Almaden's 
WebFountain project and describes an optimization model for controlling the 
crawl strategy. This crawler is scalable and incremental. The model makes 
no assumptions about the statistical behaviour of web page changes, but 
rather uses an adaptive approach to maintain data on actual change rates 
which are in turn used as inputs for the optimization. Computational 
results with simulated but realistic data show that there is no "magic 
bullet" - different, but equally plausible, objectives lead to conflicting 
"optimal" strategies. However, we find that there are compromise objectives 
which lead to good strategies that are robust against a number of criteria."               

  
     <hr width=44% align=left> Kevin S. McCurley: 

<a href="library/Geospatial&#32;Mapping&#32;and&#32;Navigation&#32;of&#32;the&#32;Web.htm">
Geospatial Mapping and Navigation of the Web</a> (2001)<br>                    
"Web pages may be organized, indexed, searched, and navigated along several 
different feature dimensions. We investigate different approaches to discovering 
geographic context for web pages, and describe 
a navigational tool for browsing web resources by geographic proximity"

     <hr width=44% align=left> Ziv Bar-Yossef, Sridhar Rajagopalan: 

<a href="library/Template&#32;Detection&#32;via&#32;Data&#32;Mining&#32;and&#32;its&#32;Applications.htm">
Template Detection via Data Mining and its Applications</a> (2001)<br>
     "We formulate and propose the template detection problem, and suggest a practical 
solution for it based on counting frequent item sets. We show that the use of templates 
is pervasive on the web. We describe three principles, which characterize the assumptions
 made by hypertext information retrieval (IR) and data mining (DM) systems, 
and show that templates are a major source of violation of these principles. 
As a consequence, basic ``pure'' implementations of simple search algorithms 
coupled with template detection and elimination 
show surprising increases in precision at all levels of recall" 
 
  <hr width=44% align=left>
Sriram Raghavan Hector Garcia Molina :
<a target=_top href="http://www.n3labs.com/pdf/www10poster.html">
Crawling the Hidden Web (Extended Abstract)</a> (2001)<br> 
"Current-day crawlers retrieve content from the publicly in-
dexable Web, i.e., the set of web pages reachable purely by 
following hypertext links, ignoring search forms and pages 
that require authorization or prior registration. In partic-
ular, they ignore the tremendous amount of high quality 
content \hidden" behind search forms, in large searchable 
electronic databases. Our work provides a framework for 
addressing the problem of extracting content from this hid-
den Web. At Stanford, we have built a task-speci c hidden 
Web crawler called the Hidden Web Exposer (HiWE). In 
this poster, we describe the architecture of HiWE and out-
line some of the novel techniques that went into its design."
  
    <hr width=44% align=left>
    <a href="latentsemantic.htm">LATENT SEMANTIC INDEXING</a>,

Taking a Holistic View<br>(2002), part of

"Patterns in Unstructured Data
Discovery, Aggregation, and Visualization"<br>











<a target=_top href-"http://javelina.cet.middlebury.edu/lsa/out/lsa_intro.htm">A Presentation to the Andrew W. Mellon Foundation</a> by

Clara Yu,

John Cuadrado,

Maciej Ceglowski,

J. Scott Payne<br><br>
"When you search an LSI-indexed database, the search engine looks
at similarity values it has calculated for every content word, and
returns the documents that it thinks best fit the query."
   
                              
                              
<!-- End Library  html -->
<pre>





  &nbsp;








</pre>
  <center>
<hr><!-- internet searching strategies and hints content, end -->

<table><tr>
<td><center><a href="basic.htm"><IMG SRC="images/basipet.jpg" 
ALT="basic" ALIGN=BOTTOM WIDTH="118" HEIGHT="68" BORDER=0 VSPACE=0 HSPACE=0></a>
<br>to basic
</center></tD>
<td><center><a href="advanced.htm"><IMG SRC="images/stillpet.jpg" 
ALT="advanced" ALIGN=BOTTOM WIDTH="118" HEIGHT="68" BORDER=0 VSPACE=0 HSPACE=0></a>
<br>to advanced
</center></tD>
<td><center><a href="classroo.htm"><IMG SRC="images/asspetit.jpg" 
ALT="classroom" ALIGN=BOTTOM WIDTH="118" HEIGHT="68" BORDER=0 VSPACE=0 HSPACE=0></a>
<br>to classroom
</center>
<td><center><a href="further.htm"><IMG SRC="images/classpet.jpg" 
ALT="further" ALIGN=BOTTOM WIDTH="118" HEIGHT="68" BORDER=0 VSPACE=0 HSPACE=0></a>
<br>to further
</center>
</td></tr></table>

 
<!-- how to search the web, by fravia+, signet begin -->
<table><tR><td width="600" height="2" bgcolor="#993300"></tD></tR></table>

(c) III Millennium: <font color=blue>[</font><a href="info.htm">fravia+</a><font color=blue><font color=blue>]</fonT></font>, all rights reserved 
<bR><!-- how to search the web, by fravia+, signet end -->


<!-- begin da closing bit, duh -->
</CENTER>
</body>
</HTML>